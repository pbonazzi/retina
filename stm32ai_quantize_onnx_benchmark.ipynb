{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <FONT COLOR=\"\"> Quantization and benchmarking of deep learning models using ONNX Runtime and STM32Cube.AI Developer Cloud : </h1>\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The process of quantization involves the convertion the original floating-point parameters and intermediate activations of a model into lower precision integer representations. This reduction in precision can significantly decrease the memory footprint and computational cost of the model, making it more efficient to deploy on STM32 board using STM32Cube.AI or any other resource-constrained devices.\n",
    "\n",
    "ONNX Runtime Quantization is a feature the ONNX Runtime that allows efficient execution of quantized models. It provides tools and techniques to quantize the ONNX format models. It includes methods for quantizing weights and activations.\n",
    "\n",
    "\n",
    "**This notebook demonstrates the process of static post-training quantization for deep learning models using the ONNX runtime. It covers the model quantization with calibration dataset or with fake data, the evaluation of the full precision model and the quantized model, and then the STM32Cube.AI Developer Cloud is used to benchmark the models and to generate the model C code to be deployed on your STM32 board.** \n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License of the Jupyter Notebook\n",
    "\n",
    "This software component is licensed by ST under BSD-3-Clause license,\n",
    "the \"License\"; \n",
    "\n",
    "You may not use this file except in compliance with the\n",
    "License. \n",
    "\n",
    "You may obtain a copy of the License at: https://opensource.org/licenses/BSD-3-Clause\n",
    "\n",
    "Copyright (c) 2023 STMicroelectronics. All rights reserved"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid #273B5F\">\n",
    "<h2>Table of content</h2>\n",
    "<ul style=\"list-style-type: none\">\n",
    "  <li><a href=\"#settings\">1. Settings</a>\n",
    "  <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#install\">1.1 Install and import necessary packages</a></li>\n",
    "    <li><a href=\"#select\">1.2 Select input model filename and dataset folder</a></li>\n",
    "  </ul>\n",
    "</li>\n",
    "<li><a href=\"#quantization\">2.Quantization</a></li>\n",
    "      <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#opset\">2.1 Opset conversion</a></li>\n",
    "    <li><a href=\"#dataset\">2.2 Creating calibration dataset</a></li>\n",
    "    <li><a href=\"#quantize\">2.3 Quantize the model using QDQ quantization to int8 weights and activations</a></li>\n",
    "  </ul>\n",
    "<li><a href=\"#Model validation\">3. Model validation </a></li>\n",
    "<li><a href=\"#benchmark_both\">4. Benchmarking the Models on the STM32Cube.AI Developer Cloud</a></li>\n",
    "      <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#proxy\">4.1 Proxy setting and connection to the STM32Cube.AI Developer Cloud</a></li>\n",
    "    <li><a href=\"#Benchmark_both\">4.2 Benchmark the models on a STM32 target</a></li>\n",
    "    <li><a href=\"#generate\">4.2 Generate the model optimized C code for STM32</a></li>\n",
    "         \n",
    "\n",
    "  </ul>\n",
    "</ul>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"settings\">\n",
    "    <h2>1. Settings</h2>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"install\">\n",
    "    <h3>1.1 Install and import necessary packages </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy==1.23.5 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (1.23.5)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: onnx==1.15.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (1.15.0)\n",
      "Requirement already satisfied: numpy in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from onnx==1.15.0) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from onnx==1.15.0) (4.25.6)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: onnxruntime==1.18.1 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (1.18.1)\n",
      "Requirement already satisfied: coloredlogs in /home/pbonazzi/.local/lib/python3.10/site-packages (from onnxruntime==1.18.1) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from onnxruntime==1.18.1) (25.2.10)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.6 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from onnxruntime==1.18.1) (1.23.5)\n",
      "Requirement already satisfied: packaging in /home/pbonazzi/.local/lib/python3.10/site-packages (from onnxruntime==1.18.1) (23.1)\n",
      "Requirement already satisfied: protobuf in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from onnxruntime==1.18.1) (4.25.6)\n",
      "Requirement already satisfied: sympy in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from onnxruntime==1.18.1) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime==1.18.1) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from sympy->onnxruntime==1.18.1) (1.3.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tensorflow==2.15.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (4.25.6)\n",
      "Requirement already satisfied: setuptools in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorflow==2.15.0) (1.50.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorflow==2.15.0) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.3.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/pbonazzi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pbonazzi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pbonazzi/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/pbonazzi/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scikit-learn in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: Pillow==9.4.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (9.4.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: matplotlib in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (3.9.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/pbonazzi/.local/lib/python3.10/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from matplotlib) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pbonazzi/.local/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/pbonazzi/.local/lib/python3.10/site-packages (from matplotlib) (2.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/pbonazzi/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from cycler>=0.10->matplotlib) (1.17.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tqdm in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (4.67.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: marshmallow in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: backports-datetime-fromisoformat in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from marshmallow) (2.0.3)\n",
      "Requirement already satisfied: typing-extensions in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from marshmallow) (4.12.2)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gitdir in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (1.2.7)\n",
      "Requirement already satisfied: colorama~=0.4 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from gitdir) (0.4.6)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gitdir in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (1.2.7)\n",
      "Requirement already satisfied: colorama~=0.4 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages (from gitdir) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy==1.23.5\n",
    "!{sys.executable} -m pip install onnx==1.15.0\n",
    "!{sys.executable} -m pip install onnxruntime==1.18.1\n",
    "!{sys.executable} -m pip install tensorflow==2.15.0\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install Pillow==9.4.0\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install marshmallow \n",
    "\n",
    "# for the cloud service\n",
    "!{sys.executable} -m pip install gitdir\n",
    "# for the cloud service\n",
    "!{sys.executable} -m pip install gitdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 17:26:00.678158: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-23 17:26:00.698444: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-23 17:26:00.698467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 17:26:00.699153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-23 17:26:00.702959: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-23 17:26:01.051368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnx import version_converter\n",
    "from onnxruntime import quantization\n",
    "from onnxruntime.quantization import (CalibrationDataReader, CalibrationMethod, QuantFormat, QuantType, quantize_static)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"select\">\n",
    "    <h3>1.2 Select input folder</h3>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir =\"./output/retina-ann-trained-50\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"quantization\">\n",
    "    <h2>2. Quantization</h2>\n",
    "</div>\n",
    "\n",
    "<div id=\"opset\">\n",
    "    <h3>2.1. Opset conversion  </h3>\n",
    "</div>\n",
    "\n",
    "In this section, we are upgrading the model's opset to version 15 to take advantage of advanced optimizations such as Batch normalization folding and ensure compatibility with the latest versions of ONNX and ONNX runtime. To do this, we run the code below.\n",
    "\n",
    "To ensure compatibility between the ONNX runtime version and the opset number, please refer to [the official documentation of ONNX Runtime](https://onnxruntime.ai/docs/reference/compatibility.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been converted to opset 15 and saved at the same location.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./output/retina-ann-trained-50/models/model-simplified.onnx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_opset(input_model: str, new_opset: int) -> str:\n",
    "    \"\"\"\n",
    "    Converts the opset version of an ONNX model to a new opset version.\n",
    "\n",
    "    Args:\n",
    "        input_model (str): The path to the input ONNX model.\n",
    "        new_opset (int): The new opset version to convert the model to.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the converted ONNX model.\n",
    "    \"\"\"\n",
    "    if not input_model.endswith('.onnx'):\n",
    "        raise Exception(\"Error! The model must be in onnx format\")    \n",
    "    model = onnx.load(input_model)\n",
    "    # Check the current opset version\n",
    "    current_opset = model.opset_import[0].version\n",
    "    if current_opset == new_opset:\n",
    "        print(f\"The model is already using opset {new_opset}\")\n",
    "        return input_model\n",
    "\n",
    "    # Modify the opset version in the model\n",
    "    converted_model = version_converter.convert_version(model, new_opset)\n",
    "    temp_model_path = input_model+ '.temp'\n",
    "    onnx.save(converted_model, temp_model_path)\n",
    "\n",
    "    # Load the modified model using ONNX Runtime Check if the model is valid\n",
    "    session = onnxruntime.InferenceSession(temp_model_path)\n",
    "    try:\n",
    "        session.get_inputs()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the modified model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Replace the original model file with the modified model\n",
    "    os.replace(temp_model_path, input_model)\n",
    "    print(f\"The model has been converted to opset {new_opset} and saved at the same location.\")\n",
    "    return input_model\n",
    "\n",
    "input_model_path = os.path.join(input_dir, \"models\", \"model-simplified.onnx\")\n",
    "change_opset(input_model_path, new_opset=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"dataset\">\n",
    "    <h3> 2.2 Creating the calibration dataset </h3>\n",
    "</div>\n",
    "\n",
    "During ONNX runtime quantization, the model is run on the calibration data to provide statistics about the dynamic and characteristics of each input and output. These statistics are then used to determine the main quantization parameters, which are the scale factor and a zero-point or offset to map the floating-point values to integers.\n",
    "\n",
    "The next three code sections below contain:\n",
    "\n",
    "* The `create_calibration_dataset` function to create the calibration set from the original directory by taking a specific number of samples from each class, and the `preprocess_image_batch` function to load the batch and process it.\n",
    "* The `preprocess_random_images` function to generate random images for fake quantization and preprocess them.\n",
    "* The `ImageNetDataReader` class that inherits from the ONNX Runtime calibration data readers and implements the `get_next` method to generate and provide input data dictionaries for the calibration process.\n",
    "\n",
    "**Note:** Using a different normalization method during quantization than during training can affect the scale of the data and lead to a loss of accuracy in the quantized model. For example, if you used TensorFlow's normalization method during training, where the data is scaled by dividing each pixel value by 255.0, you should also use this method during quantization. Similarly, if you used Torch's normalization method during training, where the data is scaled by subtracting the mean and dividing by the standard deviation, you should also use this method during quantization.\n",
    "\n",
    "Using the same normalization method for both training and quantization ensures that the quantized model retains the accuracy achieved during training. Therefore, it is important to pay attention to the normalization method used during both training and quantization to ensure the best possible accuracy for your model.\n",
    "\n",
    "To align the preprocessing of the quantization dataset in the section below with the preprocessing of the trained model, adjust the arguments `color_mode`, `interpolation`, and `norm` for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbonazzi/miniconda3/envs/retina/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch of data: torch.Size([1, 10, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "from data.module import EyeTrackingDataModule \n",
    "from data.utils import load_yaml_config\n",
    "\n",
    "# Representative dataset function for calibration\n",
    "class EyeTrackingDataReader(CalibrationDataReader):\n",
    "    \"\"\"\n",
    "    A class used to provide a representative dataset for calibration.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    train_dataloader : DataLoader\n",
    "        The training data loader\n",
    "    enum_data : iter\n",
    "        Enumerator for iterating through the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, train_dataloader: DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the RepresentativeDataset class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_dataloader : DataLoader\n",
    "            The data loader for training data\n",
    "        \"\"\"\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.enum_data = None  # Enumerator for calibration data \n",
    "        \n",
    "        try:\n",
    "            first_batch = next(iter(self.train_dataloader))\n",
    "            print(\"First batch of data:\", first_batch[0].shape)  # Print the shape of the first batch\n",
    "        except StopIteration:\n",
    "            print(\"train_dataloader is empty!\")\n",
    "            \n",
    "        # Use inference session to get input shape\n",
    "        session = onnxruntime.InferenceSession(model_path, None)\n",
    "        (_, channel, height, width) = session.get_inputs()[0].shape\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "\n",
    "    def get_next(self) -> list:\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = self._create_enumerator()\n",
    "\n",
    "        data = next(self.enum_data, None)\n",
    "        if data is None:\n",
    "            print(\"No data returned!\") \n",
    "        return data\n",
    "\n",
    "    def rewind(self) -> None:\n",
    "        \"\"\"\n",
    "        Resets the enumeration of the dataset.\n",
    "        \"\"\"\n",
    "        self.enum_data = None  # Reset the enumerator for the dataset\n",
    "\n",
    "    def _create_enumerator(self):\n",
    "        \"\"\"\n",
    "        Creates an iterator that generates representative dataset items.\n",
    "\n",
    "        Yields\n",
    "        -------\n",
    "        list\n",
    "            A list containing the input data for calibration\n",
    "        \"\"\"\n",
    "        for input_data, _, _ in self.train_dataloader:\n",
    "            input_data = input_data.detach().cpu().numpy().astype(np.float32)\n",
    "            for i in range(input_data.shape[0]): \n",
    "                yield {self.input_name: input_data[i]} \n",
    "                \n",
    "# Load dataset params\n",
    "training_params = load_yaml_config(os.path.join(input_dir, \"training_params.yaml\"))\n",
    "dataset_params = load_yaml_config(os.path.join(input_dir, \"dataset_params.yaml\"))  \n",
    "training_params[\"batch_size\"] = 1\n",
    "data_module = EyeTrackingDataModule(dataset_params=dataset_params, training_params=training_params, num_workers=16)\n",
    "data_module.setup(stage='fit')\n",
    "\n",
    "sampler = RandomSampler(data_module.train_dataset, replacement=True, num_samples=64)\n",
    "train_dataloader = data_module.train_dataloader(sampler)\n",
    "data_reader = EyeTrackingDataReader(input_model_path, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header C File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set the save path\n",
    "save_path = os.path.join(\"output\", \"sample_batches.h\")\n",
    "num_batches_to_save = 5\n",
    "saved_batches = []\n",
    "\n",
    "# Save batches from DataLoader\n",
    "for i, (input_data, target, _) in enumerate(train_dataloader):\n",
    "    if i >= num_batches_to_save:\n",
    "        break\n",
    "    # Detach and convert to numpy\n",
    "    np_input = input_data.detach().cpu().numpy()\n",
    "    saved_batches.append(np_input)\n",
    "\n",
    "# Write to C header\n",
    "with open(save_path, \"w\") as f:\n",
    "    f.write(\"#ifndef SAMPLE_BATCHES_H\\n\")\n",
    "    f.write(\"#define SAMPLE_BATCHES_H\\n\\n\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(saved_batches):\n",
    "        batch = batch.astype(np.float32)\n",
    "        flat_data = batch.flatten()\n",
    "        shape = batch.shape  # (B, C, H, W)\n",
    "\n",
    "        f.write(f\"// Batch {batch_idx}, shape: {shape}\\n\")\n",
    "        f.write(f\"static const float sample_batch_{batch_idx}[] = {{\\n\")\n",
    "        for i, value in enumerate(flat_data):\n",
    "            f.write(f\"{value:.6f}f\")\n",
    "            if i < len(flat_data) - 1:\n",
    "                f.write(\", \")\n",
    "            if (i + 1) % 8 == 0:\n",
    "                f.write(\"\\n\")\n",
    "        f.write(\"\\n};\\n\\n\")\n",
    "\n",
    "    f.write(\"#endif // SAMPLE_BATCHES_H\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"quantize\">\n",
    "    <h3> 2.3 Quantize the model using QDQ quantization to int8 weights and activations </h3>\n",
    "</div>\n",
    "\n",
    "The following section quantize the float32 onnx model to int8 quantized onnx model after the preprocessing to prepare it to the qunatization by using the ``quantize_static`` function that we recommand to use with calibration data and with the following supported arguments setting.\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: left\">Argument</th>\n",
    "<th style=\"text-align: left\">Description /  CUBE.AI recommendation</th>\n",
    "</tr>\n",
    "    \n",
    "<tr><td style=\"text-align: left\">Quant_format </td>\n",
    "<td style=\"text-align: left\"> <p> QuantFormat.QDQ format: <strong>recommended</strong>, it quantizes the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor. QOperator format: <strong> not recommended </strong>, it quantizes the model with quantized operators directly </p> </td></tr>\n",
    "<tr><td style=\"text-align: left\"> Activation type</td> \n",
    "<td style=\"text-align: left\"> <p> QuantType.QInt8: <strong>recommended</strong>, it quantizes the activations to int8.  QuantType.QUInt8: <strong>not recommended</strong>, to quantize the activations uint8 </p> </td></tr>  \n",
    "<tr><td style=\"text-align: left\">Weight_type </td> \n",
    "<td style=\"text-align: left\"> <p> QuantType.QInt8: <strong>recommended</strong> , it quantizes the weights to int8.  QuantType.QUInt8: <strong>not recommended</strong>, it quantizes the weights to uint8</p> </td></tr> \n",
    "<tr><td style=\"text-align: left\">Per_Channel</td>\n",
    "<td style=\"text-align: left\"> <p>True: <strong>recommended</strong>, it makes the quantization process is carried out individually and separately for each channel based on the characteristics of the data within that specific channel / False: supported and <strong>not recommended</strong>, the quantization process is carried out for each tensor </p> </td>\n",
    "</tr>\n",
    "<tr><td style=\"text-align: left\">ActivationSymmetric</td>\n",
    "<td style=\"text-align: left\"> <p>False: <strong>recommended</strong> it makes the activations in the range of [-128  +127]. True: supported, it makes the  activations in the range of [-128  +127] with the zero_point=0 </p> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left\">WeightSymmetric</td>\n",
    "<td style=\"text-align: left\"> <p>True: <strong>Highly recommended</strong>, it makes the weights in the range of [-127  +127] with the zero_point=0.  False: supported and <strong>not recommended</strong>, it makes the weights in the range of [-128  +127]</p> </td>\n",
    "</tr>\n",
    "<td style=\"text-align: left\">reduce_range</td>\n",
    "<td style=\"text-align: left\"> <p>True: <strong>Highly recommended</strong>, it quantizes the weights in 7-bits. It may improve the accuracy for some models, especially for per-channel mode</p> </td>\n",
    "</tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer for the model: model-simplified.onnx...\n",
      "Quantize the model model-simplified.onnx, please wait...\n",
      "No data returned!\n",
      "17:26:34 - model-simplified_QDQ_quant.onnx model has been created.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the model to infer shapes of each tensor\n",
    "infer_model = os.path.splitext(input_model_path)[0] + '_infer' + os.path.splitext(input_model_path)[1]\n",
    "print('Infer for the model: {}...'.format(os.path.basename(input_model_path)))\n",
    "quantization.quant_pre_process(input_model_path=input_model_path, output_model_path=infer_model, skip_optimization=False)\n",
    "\n",
    "# Prepare quantized ONNX model filename\n",
    "quant_model = os.path.splitext(input_model_path)[0] + '_QDQ_quant' + os.path.splitext(input_model_path)[1] \n",
    "print('Quantize the model {}, please wait...'.format(os.path.basename(input_model_path)))\n",
    "\n",
    "quantize_static(\n",
    "        infer_model,\n",
    "        quant_model,\n",
    "        data_reader,\n",
    "        calibrate_method=CalibrationMethod.MinMax, \n",
    "        quant_format=QuantFormat.QDQ,\n",
    "        per_channel=True,\n",
    "        weight_type=QuantType.QInt8, \n",
    "        activation_type=QuantType.QInt8, \n",
    "        reduce_range=True,\n",
    "        extra_options={\n",
    "        'WeightSymmetric': True,\n",
    "        'ActivationSymmetric': False,\n",
    "        'AddQDQPairToInput': False,  \n",
    "        'AddQDQPairToOutput': False  \n",
    "    })\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(current_time + ' - ' + '{} model has been created.'.format(os.path.basename(quant_model)))\n",
    "quantized_session = onnxruntime.InferenceSession(quant_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"validation\">\n",
    "        <h2> 3. Model validation </h2>\n",
    "</div>\n",
    "\n",
    "The following code section includes functions to evaluate the models on the validation dataset. It's important to note that the preprocessing of the evaluation dataset should match the preprocessing of the data during training and quantization. Therefore, make sure to adjust the arguments ``color_mode``, ``interpolation``, and ``norm`` to correspond to your preprocessing during the training scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import ModelProto\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_preprocessed_image(image_path: str, height: int, width: int, color_mode: str, interpolation: str, norm: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocesses an image for input to a neural network.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "        height (int): The desired height of the image.\n",
    "        width (int): The desired width of the image.\n",
    "        color_mode (str): The color mode of the image ('rgb' or 'rgba').\n",
    "        interpolation (str): The interpolation method to use when resizing the image.\n",
    "        norm (str): The normalization method to use ('tf' or 'torch').\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The preprocessed image as a numpy array.\n",
    "    \"\"\"\n",
    "    TORCH_MEANS = [0.485,0.456,0.406]\n",
    "    TORCH_STD = [0.224, 0.224, 0.224]\n",
    "\n",
    "    img = tf.keras.utils.load_img(image_path, color_mode = color_mode,\n",
    "     target_size = (width,height), interpolation=interpolation)\n",
    "    img_array = np.array([tf.keras.utils.img_to_array(img)])\n",
    "    if norm.lower() == 'tf':\n",
    "        img_array = -1 + img_array / 127.5\n",
    "    elif norm.lower() == 'torch':\n",
    "        img_array = img_array / 255.0\n",
    "        img_array = img_array - TORCH_MEANS\n",
    "        img_array= img_array/ TORCH_STD\n",
    "    img_array = img_array.transpose((0,3,1,2))\n",
    "    return img_array\n",
    "\n",
    "def predict_onnx(sess: ModelProto, data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Runs inference on an ONNX model.\n",
    "\n",
    "    Args:\n",
    "        sess (ModelProto): The ONNX model.\n",
    "        data (np.ndarray): The input data for the model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The model's predictions.\n",
    "    \"\"\"\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    label_name = sess.get_outputs()[0].name\n",
    "    onx_pred = sess.run([label_name], {input_name: data.astype(np.float32)})[0]\n",
    "    return onx_pred\n",
    "\n",
    "def plot_confusion_matrix(cm: np.ndarray, class_labels: List[str], model_name: str, val_accuracy: float = None) -> None:\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        cm (np.ndarray): The confusion matrix.\n",
    "        class_labels (List[str]): The labels for the classes.\n",
    "        model_name (str): The name of the model.\n",
    "        val_accuracy (float, optional): The validation accuracy of the model. Defaults to None.\n",
    "    \"\"\"\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, pad=0.1)\n",
    "\n",
    "    # Show all ticks\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_labels, yticklabels=class_labels,\n",
    "           title=f'Model Accuracy: {val_accuracy} %',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_normalized.max() / 2.\n",
    "    for i in range(cm_normalized.shape[0]):\n",
    "        for j in range(cm_normalized.shape[1]):\n",
    "            ax.text(j, i, format(cm_normalized[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm_normalized[i, j] > thresh else \"black\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'outputs/{model_name}_confusion-matrix.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_onnx_model(onnx_model_path: str, val_dir: str, model_name: str, interpolation: str = 'bilinear') -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluates an ONNX model on a validation dataset.\n",
    "\n",
    "    Args:\n",
    "        onnx_model_path (str): The path to the ONNX model.\n",
    "        val_dir (str): The path to the validation dataset.\n",
    "        model_name (str): The name of the model.\n",
    "        interpolation (str, optional): The interpolation method to use when resizing images. Defaults to 'bilinear'.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, np.ndarray]: The validation accuracy and confusion matrix.\n",
    "    \"\"\"\n",
    "    onx = ModelProto()\n",
    "    with open(onnx_model_path, mode='rb') as f:\n",
    "        content = f.read()\n",
    "        onx.ParseFromString(content)\n",
    "    sess = onnxruntime.InferenceSession(onnx_model_path)\n",
    "    (_, _, img_height, img_width) = sess.get_inputs()[0].shape\n",
    "    gt_labels = []\n",
    "    prd_labels = np.empty((0))\n",
    "    class_labels = sorted(os.listdir(val_dir))\n",
    "    for i in range(len(class_labels)):\n",
    "        class_label = class_labels[i]\n",
    "        \n",
    "        for file in os.listdir(os.path.join(val_dir, class_label)):\n",
    "            gt_labels.append(i)\n",
    "            image_path = os.path.join(val_dir, class_label, file)\n",
    "            # don't forget to adapt the preprocessing schema\n",
    "            img = get_preprocessed_image(image_path, width=img_width, height=img_height, \n",
    "                                          color_mode='rgb',interpolation=interpolation, norm='tf')\n",
    "            # predicting the results on the batch\n",
    "            pred = predict_onnx(sess, img).argmax(axis=1)\n",
    "            prd_labels = np.concatenate((prd_labels, pred))\n",
    "\n",
    "    val_acc = round(accuracy_score(gt_labels, prd_labels) * 100, 2)\n",
    "    print(f'Evaluation Top 1 accuracy: {val_acc} %')\n",
    "    if not os.path.exists(\"outputs\"):\n",
    "        os.makedirs(\"outputs\")\n",
    "    log_file_name = \"outputs/\" + model_name + \".log\"\n",
    "    with open(log_file_name, 'a') as f:\n",
    "        f.write(\"Evaluation Top 1 accuracy: {} %\\n\".format(val_acc))\n",
    "    val_cm = confusion_matrix(gt_labels, prd_labels)\n",
    "    plot_confusion_matrix(val_cm, class_labels, model_name, val_accuracy=val_acc)\n",
    "    \n",
    "    return val_acc, val_cm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Float model validation:**\n",
    "\n",
    "We evaluate the full precision model to provide a baseline measure of the model's accuracy in its original form when weights, activations, and computations are represented as 32-bit floating-point numbers without any quantization applied.\n",
    "\n",
    "To evaluate the float model, set the `val_set` variable to the path of the evaluation dataset, `input_model` to the path of the float model and the `model_name`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = \"path/to/val_set\"\n",
    "input_model = \"models/mobilenet_v2_128_0.5.onnx\"\n",
    "model_name = 'mobilenet_v2_128_0.5'\n",
    "evaluate_onnx_model(input_model, val_set, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quantized model validation:**\n",
    "\n",
    "To evaluate the quantized model, set the  `quantized_model_path` to the path of the quantized model and the `model_name`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model = \"models/mobilenet_v2_128_0.5_QDQ_quant.onnx\"\n",
    "model_name = 'mobilenet_v2_128_0.5_QDQ_quant'\n",
    "evaluate_onnx_model(input_model, val_set, model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"benchmark\">\n",
    "        <h2> 4. Benchmarking the Models on the STM32Cube.AI Developer Cloud</h2>\n",
    "</div>\n",
    "\n",
    "In this section, we use the [STM32Cube.AI Developer Cloud](https://stedgeai-dc.st.com/home) to optimize and benchmark a quantized neural network on an **STM32** target and generate its code for deployment.\n",
    "\n",
    "<div id=\"proxy\">\n",
    "        <h3> 4.1 Proxy Settings and Connection to the STM32Cube.AI Developer Cloud</h3>\n",
    "</div>\n",
    "\n",
    "If you are behind a proxy, you can uncomment and fill in the following proxy settings.\n",
    "\n",
    "**Note:** If the password contains special characters such as `@`, `:`, etc., they need to be URL-encoded with their ASCII values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['http_proxy'] = \"http://user:passwd@ip_address:port\"\n",
    "# os.environ['https_proxy'] = \"https://user:passwd@ip_address:port\"\n",
    "## And eventually disable SSL verification\n",
    "# os.environ['NO_SSL_VERIFY'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To successfully connect to the [STM32Cube.AI Developer Cloud](https://stedgeai-dc.st.com/home) you need to `gitdir` the [`STM32AI Python interface`](https://github.com/STMicroelectronics/stm32ai-modelzoo_services/tree/main/common/stm32ai_dc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get STM32Cube.AI Developer Cloud\n",
    "!gitdir https://github.com/STMicroelectronics/stm32ai-modelzoo_services/tree/main/common/stm32ai_dc\n",
    "\n",
    "# Reorganize local folders\n",
    "if os.path.exists('./stm32ai_dc'):\n",
    "    shutil.rmtree('./stm32ai_dc')\n",
    "shutil.move('./common/stm32ai_dc', './stm32ai_dc')\n",
    "shutil.rmtree('./common')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(os.path.abspath('stm32ai'))\n",
    "os.environ['STATS_TYPE'] = 'jupyter_devcloud'\n",
    "\n",
    "from stm32ai_dc import (CliLibraryIde, CliLibrarySerie, CliParameters,\n",
    "                        CloudBackend, Stm32Ai)\n",
    "from stm32ai_dc.errors import BenchmarkServerError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an account on **myST** and then sign in to [STM32Cube.AI Developer Cloud](https://stedgeai-dc.st.com/home) to be able access the service and then set the environment variables below with your credentials; the mail adress should be set as a string in username and a popup will appear to enter the password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "username ='xxx.yyy@st.com'\n",
    "os.environ['stmai_username'] = username\n",
    "print('Enter you password')\n",
    "password = getpass.getpass()\n",
    "os.environ['stmai_password'] = password\n",
    "os.environ['NO_SSL_VERIFY'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log in STM32Cube.AI Developer Cloud \n",
    "try:\n",
    "    stmai = Stm32Ai(CloudBackend(str(username), str(password)))\n",
    "    print(\"Successfully Connected!\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"benchmark_both\">\n",
    "        <h3> 4.2 Benchmark the models on a STM32 target</h3>\n",
    "</div>\n",
    "\n",
    "Then, run the code section below for later usage for the benchmark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_footprints_and_inference_time(report: object, model_name: str, board_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Analyzes the inference time of a STM32Cube.AI model and saves the results in a log file.\n",
    "\n",
    "    Args:\n",
    "        report (object): The report object containing the inference time information.\n",
    "        model_name (str): The name of the model being analyzed.\n",
    "        board_name (str): The name of the board on which the model is being analyzed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    activations_ram = report.ram_size / 1024\n",
    "    weights_rom = report.rom_size / 1024\n",
    "    macc = report.macc / 1e6\n",
    "    cycles = report.cycles\n",
    "    inference_time = report.duration_ms\n",
    "    fps = 1000.0/inference_time\n",
    "\n",
    "    print(\"[INFO] : Benchmarking the model on the {} board\\n\".format(board_name))\n",
    "    print(\"[INFO] : MACCs : {} (M)\".format(macc))\n",
    "    print(\"[INFO] : Flash Weights  : {0:.1f} (KiB)\".format(weights_rom))\n",
    "    print(\"[INFO] : RAM Activations : {0:.1f} (KiB)\".format(activations_ram))\n",
    "    print(\"[INFO] : Number of cycles : {} \".format(cycles))\n",
    "    print(\"[INFO] : Inference Time : {0:.1f} (ms)\".format(inference_time))\n",
    "    print(\"[INFO] : FPS : {0:.1f}\".format(fps))\n",
    "\n",
    "    # Writing to log file\n",
    "    model_name_without_extension = model_name.replace(\".onnx\", \"\")\n",
    "    log_file_name = \"outputs/\" + model_name_without_extension + \".log\"\n",
    "    with open(log_file_name, 'a') as f:\n",
    "        f.write(\"[INFO] : Benchmarking the model on the {} board\\n\".format(board_name))\n",
    "        f.write(\"[INFO] : Model Name : {}\\n\".format(model_name))\n",
    "        f.write(\"[INFO] : MACCs : {} (M)\\n\".format(macc))\n",
    "        f.write(\"[INFO] : Flash Weights  : {0:.1f} (KiB)\\n\".format(weights_rom))\n",
    "        f.write(\"[INFO] : RAM Activations : {0:.1f} (KiB)\\n\".format(activations_ram))\n",
    "        f.write(\"[INFO] : Number of cycles : {}\\n\".format(cycles))\n",
    "        f.write(\"[INFO] : Inference Time : {0:.1f} (ms)\\n\".format(inference_time))\n",
    "        f.write(\"[INFO] : FPS : {0:.1f}\\n\".format(fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark the float model:** \n",
    "\n",
    "The next step to benchmark the model is to upload the model on STM32Cube.AI Developer Cloud by running the code below\n",
    "\n",
    "The code above is used to upload the model to the STM32Cube.AI Developer Cloud and benchmark it on a specific STM32 board. The `model_path` variable specifies the path to the ONNX model file. The `board_name` variable specifies the name of the STM32 board on which the model will be benchmarked. \n",
    "\n",
    "Then the model is benchmarked on the specified board and generate a report of the inference time and memory footprint. The following table lists the available options the **8.1.0** of STM32Cube.AI and their descriptions for the benchmark on the STM32 boards:\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: left\">Option</th>\n",
    "<th style=\"text-align: left\">Description /  CUBE.AI recommendation</th>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "    \n",
    "    \n",
    "<td style=\"text-align: left\">model</td>\n",
    "<td style=\"text-align: left\">model name corresponding to the file name uploaded</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">optimization</td>\n",
    "<td style=\"text-align: left\">optimization setting \"balanced\", \"time\" or \"ram\"</td>\n",
    "</tr>\n",
    "    \n",
    "<tr>\n",
    "<td style=\"text-align: left\">allocateInputs</td>\n",
    "<td style=\"text-align: left\"><strong>recommended</strong>, activations buffer will be also used to handle the input buffers.True by default</td>\n",
    "</tr>\n",
    " \n",
    "<tr>\n",
    "<td style=\"text-align: left\">allocateOutputs</td>\n",
    "<td style=\"text-align: left\"><strong>recommended</strong>, activations buffer will be also used to handle the output buffers. True by default</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">relocatable</td>\n",
    "<td style=\"text-align: left\"><strong>recommended</strong>, to generate a relocatable binary model. '--binary' option can be used to have a separate binary file with only the data of the weight/bias tensors. True by default</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">noOnnxOptimizer</td>\n",
    "<td style=\"text-align: left\"><strong>not recommended</strong>, allows to disable the ONNX optimizer pass. \"False\" by default. Apply only to ONNX file will be ignored otherwise</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td style=\"text-align: left\">noOnnxIoTranspose</td>\n",
    "<td style=\"text-align: left\"> <strong>recommended only if</strong> the onnx model has already IO transpose layers to make it expect channel last data, allows to avoid adding a specific transpose layer during the import of a ONNX model, \"False\" by default. Apply only to ONNX file will be ignored otherwise</td>\n",
    "</tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/mobilenet_v2_128_0.5.onnx\"\n",
    "model_name = os.path.basename(model_path)\n",
    "try:\n",
    "  stmai.upload_model(model_path)\n",
    "  print(f'Model {model_name} is uploaded !\\n')\n",
    "except Exception as e:\n",
    "    print(\"ERROR: \", e)\n",
    "    \n",
    "board_name = 'STM32H747I-DISCO'\n",
    "result = stmai.benchmark(CliParameters(model=model_name,\n",
    "                                       optimization='balanced',\n",
    "                                       allocateInputs=True,\n",
    "                                       allocateOutputs=True,\n",
    "                                       noOnnxIoTranspose=False,\n",
    "                                       fromModel=model_name),\n",
    "                                       board_name=board_name, timeout=1500)\n",
    "\n",
    "\n",
    "analyze_footprints_and_inference_time(report=result, model_name=model_name, board_name=board_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark the int8 model:**\n",
    "\n",
    "Upload the model on STM32Cube.AI Developer Cloud and benchmark it by running the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/mobilenet_v2_128_0.5_QDQ_quant.onnx\"\n",
    "model_name = os.path.basename(model_path)\n",
    "try:\n",
    "  stmai.upload_model(model_path)\n",
    "  print(f'Model {model_name} is uploaded !\\n')\n",
    "except Exception as e:\n",
    "    print(\"ERROR: \", e)\n",
    "    \n",
    "board_name = 'STM32H747I-DISCO'\n",
    "result = stmai.benchmark(CliParameters(model=model_name,\n",
    "                                       optimization='balanced',\n",
    "                                       allocateInputs=True,\n",
    "                                       allocateOutputs=True,\n",
    "                                       noOnnxIoTranspose=False,\n",
    "                                       fromModel=model_name),\n",
    "                                       board_name=board_name)\n",
    "\n",
    "analyze_footprints_and_inference_time(report=result, model_name=model_name, board_name=board_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the next two code sections to compare the float model and the int8 model. The code will plot a figure that compares the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def compare_models(log_file_float: str, log_file_int8: str) -> None:\n",
    "    \"\"\"\n",
    "    Generates a comparison graph of two models on various metrics.\n",
    "\n",
    "    Args:\n",
    "        log_file_float: The path to the log file of the first model.\n",
    "        log_file_int8: The path to the log file of the second model.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the log files into strings\n",
    "    with open(log_file_float, 'r') as f:\n",
    "        log_float = f.read()\n",
    "    with open(log_file_int8, 'r') as f:\n",
    "        log_int8 = f.read()\n",
    "\n",
    "    # Get the metrics of interest\n",
    "    accuracy_float = float(re.search(r'Evaluation Top 1 accuracy: ([\\d.]+) %', log_float).group(1))\n",
    "    accuracy_int8 = float(re.search(r'Evaluation Top 1 accuracy: ([\\d.]+) %', log_int8).group(1))\n",
    "    flash_float = float(re.search(r'Flash\\s*Weights\\s*:\\s*([\\d.]+)\\s*\\(\\s*KiB\\s*\\)', log_float).group(1))\n",
    "    flash_int8 = float(re.search(r'Flash\\s*Weights\\s*:\\s*([\\d.]+)\\s*\\(\\s*KiB\\s*\\)', log_int8).group(1))\n",
    "    ram_float = float(re.search(r'RAM\\s*Activations\\s*:\\s*([\\d.]+)\\s*\\(\\s*KiB\\s*\\)', log_float).group(1))\n",
    "    ram_int8 = float(re.search(r'RAM\\s*Activations\\s*:\\s*([\\d.]+)\\s*\\(\\s*KiB\\s*\\)', log_int8).group(1))\n",
    "    inference_time_float = float(re.search(r'Inference\\s*Time\\s*:\\s*([\\d.]+)\\s*\\(\\s*ms\\s*\\)', log_float).group(1))\n",
    "    inference_time_int8 = float(re.search(r'Inference\\s*Time\\s*:\\s*([\\d.]+)\\s*\\(\\s*ms\\s*\\)', log_int8).group(1))\n",
    "\n",
    "    # Set the figure size and spacing between subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 8), gridspec_kw={'wspace': 0.3, 'hspace': 0.4})\n",
    "\n",
    "    # Graph 1: Accuracy Comparison\n",
    "    axs[0, 0].bar(['Float model', 'Int8 model'], [accuracy_float, accuracy_int8], color='#03234B')\n",
    "    axs[0, 0].set_title('Accuracy')\n",
    "    axs[0, 0].set_xlabel('Model')\n",
    "    axs[0, 0].set_ylabel('Accuracy (%)')\n",
    "    axs[0, 0].set_ylim([0, 100])\n",
    "    axs[0, 0].text(0, accuracy_float+1, str(round(accuracy_float, 2))+'%')\n",
    "    axs[0, 0].text(1, accuracy_int8+1, str(round(accuracy_int8, 2))+'%')\n",
    "\n",
    "    # Graph 2: RAM Activation Comparison\n",
    "    axs[0, 1].bar(['Float model', 'Int8 model'], [ram_float, ram_int8], color='#03234B')\n",
    "    axs[0, 1].set_title('RAM activation')\n",
    "    axs[0, 1].set_xlabel('Model')\n",
    "    axs[0, 1].set_ylabel('RAM activation (KiB)')\n",
    "    axs[0, 1].set_ylim([0, 1500])\n",
    "    axs[0, 1].text(0, ram_float+20, str(round(ram_float, 2))+' KiB')\n",
    "    axs[0, 1].text(1, ram_int8+20, str(round(ram_int8, 2))+' KiB')\n",
    "\n",
    "    # Graph 3: Flash Weights Comparison\n",
    "    axs[1, 0].bar(['Float model', 'Int8 model'], [flash_float, flash_int8], color='#03234B')\n",
    "    axs[1, 0].set_title('Flash Weights')\n",
    "    axs[1, 0].set_xlabel('Model')\n",
    "    axs[1, 0].set_ylabel('Flash Weights (KiB)')\n",
    "    axs[1, 0].set_ylim([0, 1000])\n",
    "    axs[1, 0].text(0, flash_float+20, str(round(flash_float, 2))+' KiB')\n",
    "    axs[1, 0].text(1, flash_int8+20, str(round(flash_int8, 2))+' KiB')\n",
    "\n",
    "    # Graph 4: Inference Time Comparison\n",
    "    axs[1, 1].bar(['Float model', 'Int8 model'], [inference_time_float, inference_time_int8], color='#03234B')\n",
    "    axs[1, 1].set_title('Inference Time')\n",
    "    axs[1, 1].set_xlabel('Model')\n",
    "    axs[1, 1].set_ylabel('Inference Time (ms)')\n",
    "    axs[1, 1].set_ylim([0, 1000])\n",
    "    axs[1, 1].text(0, inference_time_float+20, str(round(inference_time_float, 2))+' ms')\n",
    "    axs[1, 1].text(1, inference_time_int8+20, str(round(inference_time_int8, 2))+' ms')\n",
    "\n",
    "    # Set the global title\n",
    "    fig.suptitle('Comparison of Two Models on Various Metrics', fontsize=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure to a file\n",
    "    plt.savefig('comparison.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_int8 = 'outputs/mobilenet_v2_128_0.5_QDQ_quant.log'\n",
    "log_file_float = 'outputs/mobilenet_v2_128_0.5.log'\n",
    "\n",
    "compare_models(log_file_float, log_file_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"generate\">\n",
    "        <h3> 4.3 Generate the model optimized C code for STM32 </h3>\n",
    "</div>\n",
    "\n",
    "Here you generate the specialized network and data C-files to make the model ready to be integrated in the **STM32** application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "code_folder = os.path.join('outputs/code_outputs')\n",
    "os.makedirs(code_folder, exist_ok=True)\n",
    "\n",
    "board_name = 'STM32H7'\n",
    "IDE = 'gcc'\n",
    "print(f'{model_name}\\ngenerating code for {board_name}')\n",
    "\n",
    "# Generate model .c/.h code + Lib/Inc on STM32Cube.AI Developer Cloud\n",
    "result = stmai.generate(CliParameters(model=model_name,\n",
    "                                      output=code_folder,\n",
    "                                      optimization='balanced',\n",
    "                                      allocateInputs=True,\n",
    "                                      allocateOutputs=True,\n",
    "                                      noOnnxIoTranspose=False,\n",
    "                                      includeLibraryForSerie=CliLibrarySerie(board_name),\n",
    "                                      includeLibraryForIde=CliLibraryIde(IDE),\n",
    "                                      fromModel=model_name))\n",
    "\n",
    "print(os.listdir(code_folder))\n",
    "\n",
    "# Print the first 20 lines of the report\n",
    "with open(os.path.join(code_folder, 'network_generate_report.txt'), 'r') as f:\n",
    "    for _ in range(20):\n",
    "        print(next(f))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
