{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <FONT COLOR=\"\"> Quantization and benchmarking of deep learning models using ONNX Runtime and STM32Cube.AI Developer Cloud : </h1>\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "The process of quantization involves the convertion the original floating-point parameters and intermediate activations of a model into lower precision integer representations. This reduction in precision can significantly decrease the memory footprint and computational cost of the model, making it more efficient to deploy on STM32 board using STM32Cube.AI or any other resource-constrained devices.\n",
    "\n",
    "ONNX Runtime Quantization is a feature the ONNX Runtime that allows efficient execution of quantized models. It provides tools and techniques to quantize the ONNX format models. It includes methods for quantizing weights and activations.\n",
    "\n",
    "\n",
    "**This notebook demonstrates the process of static post-training quantization for deep learning models using the ONNX runtime. It covers the model quantization with calibration dataset or with fake data, the evaluation of the full precision model and the quantized model, and then the STM32Cube.AI Developer Cloud is used to benchmark the models and to generate the model C code to be deployed on your STM32 board.** \n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License of the Jupyter Notebook\n",
    "\n",
    "This software component is licensed by ST under BSD-3-Clause license,\n",
    "the \"License\"; \n",
    "\n",
    "You may not use this file except in compliance with the\n",
    "License. \n",
    "\n",
    "You may obtain a copy of the License at: https://opensource.org/licenses/BSD-3-Clause\n",
    "\n",
    "Copyright (c) 2023 STMicroelectronics. All rights reserved"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid #273B5F\">\n",
    "<h2>Table of content</h2>\n",
    "<ul style=\"list-style-type: none\">\n",
    "  <li><a href=\"#settings\">1. Settings</a>\n",
    "  <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#install\">1.1 Install and import necessary packages</a></li>\n",
    "    <li><a href=\"#select\">1.2 Select input model filename and dataset folder</a></li>\n",
    "  </ul>\n",
    "</li>\n",
    "<li><a href=\"#quantization\">2.Quantization</a></li>\n",
    "      <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#opset\">2.1 Opset conversion</a></li>\n",
    "    <li><a href=\"#dataset\">2.2 Creating calibration dataset</a></li>\n",
    "    <li><a href=\"#quantize\">2.3 Quantize the model using QDQ quantization to int8 weights and activations</a></li>\n",
    "  </ul>\n",
    "<li><a href=\"#Model validation\">3. Model validation </a></li>\n",
    "<li><a href=\"#benchmark_both\">4. Benchmarking the Models on the STM32Cube.AI Developer Cloud</a></li>\n",
    "      <ul style=\"list-style-type: none\">\n",
    "    <li><a href=\"#proxy\">4.1 Proxy setting and connection to the STM32Cube.AI Developer Cloud</a></li>\n",
    "    <li><a href=\"#Benchmark_both\">4.2 Benchmark the models on a STM32 target</a></li>\n",
    "    <li><a href=\"#generate\">4.2 Generate the model optimized C code for STM32</a></li>\n",
    "         \n",
    "\n",
    "  </ul>\n",
    "</ul>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"settings\">\n",
    "    <h2>1. Settings</h2>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"install\">\n",
    "    <h3>1.1 Install and import necessary packages </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.23.5 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (1.23.5)\n",
      "Requirement already satisfied: onnx==1.15.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (1.15.0)\n",
      "Requirement already satisfied: numpy in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from onnx==1.15.0) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from onnx==1.15.0) (4.25.7)\n",
      "Requirement already satisfied: onnxruntime==1.18.1 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (1.18.1)\n",
      "Requirement already satisfied: coloredlogs in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from onnxruntime==1.18.1) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from onnxruntime==1.18.1) (25.2.10)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.6 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from onnxruntime==1.18.1) (1.23.5)\n",
      "Requirement already satisfied: packaging in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from onnxruntime==1.18.1) (24.2)\n",
      "Requirement already satisfied: protobuf in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from onnxruntime==1.18.1) (4.25.7)\n",
      "Requirement already satisfied: sympy in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from onnxruntime==1.18.1) (1.13.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from coloredlogs->onnxruntime==1.18.1) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from sympy->onnxruntime==1.18.1) (1.3.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.15.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.15.0\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: Pillow==9.4.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (9.4.0)\n",
      "Requirement already satisfied: matplotlib in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: tqdm in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (4.67.1)\n",
      "Requirement already satisfied: marshmallow in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (3.22.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from marshmallow) (24.2)\n",
      "Requirement already satisfied: gitdir in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (1.2.7)\n",
      "Requirement already satisfied: colorama~=0.4 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from gitdir) (0.4.6)\n",
      "Requirement already satisfied: gitdir in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (1.2.7)\n",
      "Requirement already satisfied: colorama~=0.4 in /home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages (from gitdir) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy==1.23.5\n",
    "!{sys.executable} -m pip install onnx==1.15.0\n",
    "!{sys.executable} -m pip install onnxruntime==1.18.1\n",
    "!{sys.executable} -m pip install tensorflow==2.15.0\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "\n",
    "!{sys.executable} -m pip install Pillow==9.4.0\n",
    "!{sys.executable} -m pip install matplotlib\n",
    "!{sys.executable} -m pip install tqdm\n",
    "!{sys.executable} -m pip install marshmallow \n",
    "\n",
    "# for the cloud service\n",
    "\n",
    "!{sys.executable} -m pip install gitdir\n",
    "# for the cloud service\n",
    "!{sys.executable} -m pip install gitdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 11:27:06.086682: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-27 11:27:06.488948: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-27 11:27:06.491380: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 11:27:08.187713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnx import version_converter\n",
    "from onnxruntime import quantization\n",
    "from onnxruntime.quantization import (CalibrationDataReader, CalibrationMethod, QuantFormat, QuantType, quantize_static)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"select\">\n",
    "    <h3>1.2 Select input folder</h3>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir =\"./output/retina-ann-v6-evs-1000\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"quantization\">\n",
    "    <h2>2. Quantization</h2>\n",
    "</div>\n",
    "\n",
    "<div id=\"opset\">\n",
    "    <h3>2.1. Opset conversion  </h3>\n",
    "</div>\n",
    "\n",
    "In this section, we are upgrading the model's opset to version 15 to take advantage of advanced optimizations such as Batch normalization folding and ensure compatibility with the latest versions of ONNX and ONNX runtime. To do this, we run the code below.\n",
    "\n",
    "To ensure compatibility between the ONNX runtime version and the opset number, please refer to [the official documentation of ONNX Runtime](https://onnxruntime.ai/docs/reference/compatibility.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been converted to opset 15 and saved at the same location.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./output/retina-ann-v6-evs-1000/models/model.onnx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def change_opset(input_model: str, new_opset: int) -> str:\n",
    "    \"\"\"\n",
    "    Converts the opset version of an ONNX model to a new opset version.\n",
    "\n",
    "    Args:\n",
    "        input_model (str): The path to the input ONNX model.\n",
    "        new_opset (int): The new opset version to convert the model to.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the converted ONNX model.\n",
    "    \"\"\"\n",
    "    if not input_model.endswith('.onnx'):\n",
    "        raise Exception(\"Error! The model must be in onnx format\")    \n",
    "    model = onnx.load(input_model)\n",
    "    # Check the current opset version\n",
    "    current_opset = model.opset_import[0].version\n",
    "    if current_opset == new_opset:\n",
    "        print(f\"The model is already using opset {new_opset}\")\n",
    "        return input_model\n",
    "\n",
    "    # Modify the opset version in the model\n",
    "    converted_model = version_converter.convert_version(model, new_opset)\n",
    "    temp_model_path = input_model+ '.temp'\n",
    "    onnx.save(converted_model, temp_model_path)\n",
    "\n",
    "    # Load the modified model using ONNX Runtime Check if the model is valid\n",
    "    session = onnxruntime.InferenceSession(temp_model_path)\n",
    "    try:\n",
    "        session.get_inputs()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the modified model: {e}\")\n",
    "        return\n",
    "\n",
    "    # Replace the original model file with the modified model\n",
    "    os.replace(temp_model_path, input_model)\n",
    "    print(f\"The model has been converted to opset {new_opset} and saved at the same location.\")\n",
    "    return input_model\n",
    "\n",
    "input_model_path = os.path.join(input_dir, \"models\", \"model.onnx\")\n",
    "change_opset(input_model_path, new_opset=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"dataset\">\n",
    "    <h3> 2.2 Creating the calibration dataset </h3>\n",
    "</div>\n",
    "\n",
    "During ONNX runtime quantization, the model is run on the calibration data to provide statistics about the dynamic and characteristics of each input and output. These statistics are then used to determine the main quantization parameters, which are the scale factor and a zero-point or offset to map the floating-point values to integers.\n",
    "\n",
    "The next three code sections below contain:\n",
    "\n",
    "* The `create_calibration_dataset` function to create the calibration set from the original directory by taking a specific number of samples from each class, and the `preprocess_image_batch` function to load the batch and process it.\n",
    "* The `preprocess_random_images` function to generate random images for fake quantization and preprocess them.\n",
    "* The `ImageNetDataReader` class that inherits from the ONNX Runtime calibration data readers and implements the `get_next` method to generate and provide input data dictionaries for the calibration process.\n",
    "\n",
    "**Note:** Using a different normalization method during quantization than during training can affect the scale of the data and lead to a loss of accuracy in the quantized model. For example, if you used TensorFlow's normalization method during training, where the data is scaled by dividing each pixel value by 255.0, you should also use this method during quantization. Similarly, if you used Torch's normalization method during training, where the data is scaled by subtracting the mean and dividing by the standard deviation, you should also use this method during quantization.\n",
    "\n",
    "Using the same normalization method for both training and quantization ensures that the quantized model retains the accuracy achieved during training. Therefore, it is important to pay attention to the normalization method used during both training and quantization to ensure the best possible accuracy for your model.\n",
    "\n",
    "To align the preprocessing of the quantization dataset in the section below with the preprocessing of the trained model, adjust the arguments `color_mode`, `interpolation`, and `norm` for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbonazzi/miniconda3/envs/retina/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch of data: torch.Size([1, 1, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "\n",
    "from data.module import EyeTrackingDataModule \n",
    "from data.utils import load_yaml_config\n",
    "\n",
    "# Representative dataset function for calibration\n",
    "class EyeTrackingDataReader(CalibrationDataReader):\n",
    "    \"\"\"\n",
    "    A class used to provide a representative dataset for calibration.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    train_dataloader : DataLoader\n",
    "        The training data loader\n",
    "    enum_data : iter\n",
    "        Enumerator for iterating through the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str, train_dataloader: DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the RepresentativeDataset class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_dataloader : DataLoader\n",
    "            The data loader for training data\n",
    "        \"\"\"\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.enum_data = None  # Enumerator for calibration data \n",
    "        \n",
    "        try:\n",
    "            first_batch = next(iter(self.train_dataloader))\n",
    "            print(\"First batch of data:\", first_batch[0].shape)  # Print the shape of the first batch\n",
    "        except StopIteration:\n",
    "            print(\"train_dataloader is empty!\")\n",
    "            \n",
    "        # Use inference session to get input shape\n",
    "        session = onnxruntime.InferenceSession(model_path, None)\n",
    "        (_, channel, height, width) = session.get_inputs()[0].shape\n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "\n",
    "    def get_next(self) -> list:\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = self._create_enumerator()\n",
    "\n",
    "        data = next(self.enum_data, None)\n",
    "        if data is None:\n",
    "            print(\"No data returned!\") \n",
    "        return data\n",
    "\n",
    "    def rewind(self) -> None:\n",
    "        \"\"\"\n",
    "        Resets the enumeration of the dataset.\n",
    "        \"\"\"\n",
    "        self.enum_data = None  # Reset the enumerator for the dataset\n",
    "\n",
    "    def _create_enumerator(self):\n",
    "        \"\"\"\n",
    "        Creates an iterator that generates representative dataset items.\n",
    "\n",
    "        Yields\n",
    "        -------\n",
    "        list\n",
    "            A list containing the input data for calibration\n",
    "        \"\"\"\n",
    "        for input_data, _, _ in self.train_dataloader:\n",
    "            input_data = input_data.detach().cpu().numpy().astype(np.float32)\n",
    "            for i in range(input_data.shape[0]): \n",
    "                yield {self.input_name: input_data[i]} \n",
    "                \n",
    "# Load dataset params\n",
    "training_params = load_yaml_config(os.path.join(input_dir, \"training_params.yaml\"))\n",
    "dataset_params = load_yaml_config(os.path.join(input_dir, \"dataset_params.yaml\"))  \n",
    "training_params[\"batch_size\"] = 1\n",
    "data_module = EyeTrackingDataModule(dataset_params=dataset_params, training_params=training_params, num_workers=16)\n",
    "data_module.setup(stage='fit')\n",
    "\n",
    "sampler = RandomSampler(data_module.train_dataset, replacement=True, num_samples=64)\n",
    "train_dataloader = data_module.train_dataloader(sampler)\n",
    "data_reader = EyeTrackingDataReader(input_model_path, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header C File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set the save path\n",
    "save_path = os.path.join(\"output\", \"sample_batches.h\")\n",
    "num_batches_to_save = 5\n",
    "saved_batches = []\n",
    "\n",
    "# Save batches from DataLoader\n",
    "for i, (input_data, target, _) in enumerate(train_dataloader):\n",
    "    if i >= num_batches_to_save:\n",
    "        break\n",
    "    # Detach and convert to numpy\n",
    "    np_input = input_data.detach().cpu().numpy()\n",
    "    saved_batches.append(np_input)\n",
    "\n",
    "# Write to C header\n",
    "with open(save_path, \"w\") as f:\n",
    "    f.write(\"#ifndef SAMPLE_BATCHES_H\\n\")\n",
    "    f.write(\"#define SAMPLE_BATCHES_H\\n\\n\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(saved_batches):\n",
    "        batch = batch.astype(np.float32)\n",
    "        flat_data = batch.flatten()\n",
    "        shape = batch.shape  # (B, C, H, W)\n",
    "\n",
    "        f.write(f\"// Batch {batch_idx}, shape: {shape}\\n\")\n",
    "        f.write(f\"static const float sample_batch_{batch_idx}[] = {{\\n\")\n",
    "        for i, value in enumerate(flat_data):\n",
    "            f.write(f\"{value:.6f}f\")\n",
    "            if i < len(flat_data) - 1:\n",
    "                f.write(\", \")\n",
    "            if (i + 1) % 8 == 0:\n",
    "                f.write(\"\\n\")\n",
    "        f.write(\"\\n};\\n\\n\")\n",
    "\n",
    "    f.write(\"#endif // SAMPLE_BATCHES_H\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"quantize\">\n",
    "    <h3> 2.3 Quantize the model using QDQ quantization to int8 weights and activations </h3>\n",
    "</div>\n",
    "\n",
    "The following section quantize the float32 onnx model to int8 quantized onnx model after the preprocessing to prepare it to the qunatization by using the ``quantize_static`` function that we recommand to use with calibration data and with the following supported arguments setting.\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th style=\"text-align: left\">Argument</th>\n",
    "<th style=\"text-align: left\">Description /  CUBE.AI recommendation</th>\n",
    "</tr>\n",
    "    \n",
    "<tr><td style=\"text-align: left\">Quant_format </td>\n",
    "<td style=\"text-align: left\"> <p> QuantFormat.QDQ format: <strong>recommended</strong>, it quantizes the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor. QOperator format: <strong> not recommended </strong>, it quantizes the model with quantized operators directly </p> </td></tr>\n",
    "<tr><td style=\"text-align: left\"> Activation type</td> \n",
    "<td style=\"text-align: left\"> <p> QuantType.QInt8: <strong>recommended</strong>, it quantizes the activations to int8.  QuantType.QUInt8: <strong>not recommended</strong>, to quantize the activations uint8 </p> </td></tr>  \n",
    "<tr><td style=\"text-align: left\">Weight_type </td> \n",
    "<td style=\"text-align: left\"> <p> QuantType.QInt8: <strong>recommended</strong> , it quantizes the weights to int8.  QuantType.QUInt8: <strong>not recommended</strong>, it quantizes the weights to uint8</p> </td></tr> \n",
    "<tr><td style=\"text-align: left\">Per_Channel</td>\n",
    "<td style=\"text-align: left\"> <p>True: <strong>recommended</strong>, it makes the quantization process is carried out individually and separately for each channel based on the characteristics of the data within that specific channel / False: supported and <strong>not recommended</strong>, the quantization process is carried out for each tensor </p> </td>\n",
    "</tr>\n",
    "<tr><td style=\"text-align: left\">ActivationSymmetric</td>\n",
    "<td style=\"text-align: left\"> <p>False: <strong>recommended</strong> it makes the activations in the range of [-128  +127]. True: supported, it makes the  activations in the range of [-128  +127] with the zero_point=0 </p> </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: left\">WeightSymmetric</td>\n",
    "<td style=\"text-align: left\"> <p>True: <strong>Highly recommended</strong>, it makes the weights in the range of [-127  +127] with the zero_point=0.  False: supported and <strong>not recommended</strong>, it makes the weights in the range of [-128  +127]</p> </td>\n",
    "</tr>\n",
    "<td style=\"text-align: left\">reduce_range</td>\n",
    "<td style=\"text-align: left\"> <p>True: <strong>Highly recommended</strong>, it quantizes the weights in 7-bits. It may improve the accuracy for some models, especially for per-channel mode</p> </td>\n",
    "</tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer for the model: model.onnx...\n",
      "Quantize the model model.onnx, please wait...\n",
      "No data returned!\n",
      "11:27:49 - model_QDQ_quant.onnx model has been created.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the model to infer shapes of each tensor\n",
    "infer_model = os.path.splitext(input_model_path)[0] + '_infer' + os.path.splitext(input_model_path)[1]\n",
    "print('Infer for the model: {}...'.format(os.path.basename(input_model_path)))\n",
    "quantization.quant_pre_process(input_model_path=input_model_path, output_model_path=infer_model, skip_optimization=False)\n",
    "\n",
    "# Prepare quantized ONNX model filename\n",
    "quant_model = os.path.splitext(input_model_path)[0] + '_QDQ_quant' + os.path.splitext(input_model_path)[1] \n",
    "print('Quantize the model {}, please wait...'.format(os.path.basename(input_model_path)))\n",
    "\n",
    "quantize_static(\n",
    "        infer_model,\n",
    "        quant_model,\n",
    "        data_reader,\n",
    "        calibrate_method=CalibrationMethod.MinMax, \n",
    "        quant_format=QuantFormat.QDQ,\n",
    "        per_channel=True,\n",
    "        weight_type=QuantType.QInt8, \n",
    "        activation_type=QuantType.QInt8, \n",
    "        reduce_range=True,\n",
    "        extra_options={\n",
    "        'WeightSymmetric': True,\n",
    "        'ActivationSymmetric': False,\n",
    "        'AddQDQPairToInput': False,  \n",
    "        'AddQDQPairToOutput': False  \n",
    "    })\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(current_time + ' - ' + '{} model has been created.'.format(os.path.basename(quant_model)))\n",
    "quantized_session = onnxruntime.InferenceSession(quant_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
